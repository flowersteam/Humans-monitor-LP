{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T17:02:38.880345Z",
     "start_time": "2021-05-21T17:02:38.009500Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "from IPython.display import display\n",
    "from tqdm.notebook import tqdm\n",
    "from python_scripts.utils import loc_utils as lut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Combine raw data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Main data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Concatenate main raw data and codify participant IDs into a more readable form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T17:41:38.997797Z",
     "start_time": "2021-05-16T17:41:37.043483Z"
    },
    "code_folding": [],
    "hidden": true,
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "def combine_main_raw(input_data_paths, save_path, save_codes=''):\n",
    "    combine_list = []\n",
    "    cdicts = []\n",
    "    for input_data_path in input_data_paths:\n",
    "        df = pd.read_csv(input_data_path)\n",
    "        \n",
    "        # Codify subject IDs\n",
    "        uniqids = df.sid.astype('category')\n",
    "        df.loc[:, 'sid'] = uniqids.cat.codes\n",
    "        cdict = dict(enumerate(uniqids.cat.categories))\n",
    "        \n",
    "        # If not the 1st DF, continue enumerating from previous DF's last index\n",
    "        if combine_list:\n",
    "            last_index = combine_list[-1].loc[:, 'sid'].max()\n",
    "            df.loc[:, 'sid'] += last_index + 1\n",
    "            cdict = dict(enumerate(uniqids.cat.categories, last_index + 1))\n",
    "            \n",
    "        cdicts.append(cdict)    \n",
    "        combine_list.append(df)\n",
    "        \n",
    "    # Combine dataframes\n",
    "    df = pd.concat(combine_list)\n",
    "    cdict = {k:v for d in cdicts for k, v in d.items()}\n",
    "    df_codes = pd.DataFrame({'code': list(cdict.keys()), 'uid': list(cdict.values())})\n",
    "\n",
    "    # Save combined data\n",
    "    if save_path:\n",
    "        print('saving combined data to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(path.join(save_path), index=False)\n",
    "        \n",
    "    if save_codes:\n",
    "        print('saving codes to {}'.format(path.abspath(save_path)))\n",
    "        df_codes.to_csv(path.join(save_codes), index=False)\n",
    "    \n",
    "combine_main_raw(\n",
    "    input_data_paths = ('data/raw/ig_main.csv', 'data/raw/eg_main.csv'),\n",
    "    save_path = 'data/combined_main.csv',\n",
    "    save_codes = 'data/uid_codes.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Extra data files (self-reports)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Concatenate extra raw data and codify participant IDs using codes from the previous function. Then convert data to long format and clean up for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T17:41:50.821811Z",
     "start_time": "2021-05-16T17:41:50.278136Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def combine_extra_raw(input_data_paths, main_data_path, codes_path, save_path=''):\n",
    "    # Combine data and use previously generated codes\n",
    "    df = pd.concat([pd.read_csv(p) for p in input_data_paths]).set_index('sid')\n",
    "    df = df.merge(\n",
    "        pd.read_csv(codes_path).rename(columns={'uid': 'sid'}).set_index('sid'), on='sid').reset_index()\n",
    "    df.loc[:, 'sid'] = df.code\n",
    "    df.drop(columns=['code', 'age', 'gender', 'race', 'ethnicity', 'thoughts', 'comments'], inplace=True)\n",
    "    \n",
    "    # Reformat column names to convert to long format\n",
    "    rename_dict = {}\n",
    "    stubnames = []\n",
    "    for i in range(len(df.columns)):\n",
    "        s = df.columns[i]\n",
    "        if '.' in s:\n",
    "            split_str = s.split('.')\n",
    "            suffix = split_str.pop()\n",
    "            stubname = ''.join(split_str)\n",
    "            rename_dict[s] = '%'.join([stubname, suffix])\n",
    "            stubnames.append(stubname)\n",
    "    df = df.rename(columns=rename_dict)\n",
    "    \n",
    "    # Convert to long format\n",
    "    df = pd.melt(df, id_vars=['sid', 'group'], value_vars=None, var_name='item%family', value_name='rating')\n",
    "    split_cols = df.loc[:, 'item%family'].str.split('%',  expand = True)\n",
    "    split_cols.columns = ['item', 'family']\n",
    "    df = pd.concat([df, split_cols], axis=1).filter(items=['sid','group','family','item','rating'])\n",
    "    \n",
    "    # Identify activity type for each family\n",
    "    act_df = pd.read_csv(main_data_path).filter(items=['sid','family','activity']).drop_duplicates()\n",
    "    df = df.merge(act_df, on=['sid','family']).filter(items=['sid','group','activity','item','rating'])\n",
    "    \n",
    "    # Sort values and rename items\n",
    "    df = df.sort_values(by=['group','sid','item','activity']).reset_index(drop=True)\n",
    "    df.loc[:, 'item'] = df.item.replace({'futurelearn0': 'lrn1',\n",
    "                                         'futurelearn1': 'lrn2',\n",
    "                                         'interested': 'int',\n",
    "                                         'progress': 'prog',\n",
    "                                         'time': 'time',\n",
    "                                         'rule': 'rule',\n",
    "                                         'complex': 'comp'\n",
    "                                        })\n",
    "    \n",
    "    # Add normalized scores\n",
    "    mean_ratings = df.groupby(['sid']).mean().loc[:, 'rating'].reset_index().rename(columns={'rating':'norm'})\n",
    "    df = df.merge(mean_ratings, on='sid')\n",
    "    df.loc[:, 'rating_norm'] = df.rating - df.norm\n",
    "    df = df.drop(columns='norm')\n",
    "    display(df.head())\n",
    "    \n",
    "    # Save combined data\n",
    "    if save_path:\n",
    "        print('saving combined data to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(path.join(save_path), index=False)\n",
    "\n",
    "        \n",
    "\n",
    "combine_extra_raw(\n",
    "    input_data_paths = ('data/raw/ig_extra.csv', 'data/raw/eg_extra.csv'),\n",
    "    main_data_path = 'data/combined_main.csv',\n",
    "    codes_path = 'data/uid_codes.csv',\n",
    "    save_path = 'data/combined_extra.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T14:17:09.510337Z",
     "start_time": "2020-10-25T14:17:09.505646Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "# Exclude outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Exclude outliers based on allocation bias and response bias. Report number of exclusions in each group based on allocation bias, then exclude from remaining data according response bias and report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-22T00:59:50.673171Z",
     "start_time": "2021-05-22T00:59:43.280620Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def make_clean_dataset(input_data_path, save_path, **kwargs):\n",
    "    # Define a response bias function\n",
    "    def rbf(x):\n",
    "        _, response_counts = np.unique(x.response, return_counts=True)\n",
    "        return np.max(response_counts) / np.sum(response_counts)\n",
    "\n",
    "\n",
    "    # Open combined data file\n",
    "    df = pd.read_csv(input_data_path, index_col=None).set_index('sid')\n",
    "\n",
    "    # Initialize columns to record values of interest\n",
    "    df['alloc_bias'], df['resp_bias'] = 0, 0\n",
    "\n",
    "    # Calculate values of interest\n",
    "    activities = ('A1', 'A2', 'A3', 'A4')\n",
    "    for sid, sdf in tqdm(df.groupby(by='sid'), desc='Progress: '):\n",
    "        # Allocation variance\n",
    "        if kwargs['ab_crit']:\n",
    "            counts = [sum(sdf.activity == i) for i in activities]\n",
    "            allocation_variance = np.std(counts)\n",
    "            df.loc[sid, 'alloc_bias'] = allocation_variance\n",
    "        else:\n",
    "            df.loc[sid, 'alloc_bias'] = False\n",
    "\n",
    "        # Response bias\n",
    "        response_bias = sdf.groupby('family').apply(rbf).mean()\n",
    "        df.loc[sid, 'resp_bias'] = response_bias\n",
    "\n",
    "    # Detect high allocation variance and response bias\n",
    "    df_ = df.reset_index().groupby('sid').head(1).reset_index()\n",
    "    df_['high_ab'] = df_.alloc_bias >= kwargs['ab_crit']\n",
    "    df_['high_rb'] = np.logical_and(df_.resp_bias > df_.resp_bias.mean() + kwargs['rb_crit'] * df_.resp_bias.std(), ~df_.high_ab)\n",
    "\n",
    "    display(df_.groupby(by='group')[['high_ab', 'high_rb']].sum().astype(int))\n",
    "    print('Found {} outliers'.format(np.logical_or(df_.high_ab, df_.high_rb).sum()))\n",
    "\n",
    "    # Exclude outliers\n",
    "    outlier = df_.loc[df_.high_ab | df_.high_rb, 'sid']\n",
    "    df = df.loc[~df.index.isin(outlier), :] if exclude else df\n",
    "    display(df.reset_index().groupby(by='group')['sid'].nunique())\n",
    "\n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.reset_index().to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "exclude = True\n",
    "save_path = 'data/clean_data.csv' if exclude else 'data/unclean_data.csv'\n",
    "\n",
    "make_clean_dataset(\n",
    "    input_data_path = 'data/combined_main.csv',\n",
    "    save_path = save_path,\n",
    "\n",
    "    # Set outlier criteria\n",
    "    ab_crit = None,   # allocation variance critical value\n",
    "    rb_crit = 2 ,    # response bias critical value\n",
    "    \n",
    "    # Exclude outliers?\n",
    "    exclude = exclude\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Calculate heuristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Time-window approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "|Heuristic|Description ($t_i$ = trial number $i$; $w$ = window size)|\n",
    "|:-------:|:--------------------------------------------------------|\n",
    "| **PC**  | overall competence ($t_0$ to $t_i$)                     |\n",
    "| **rPC** | recent competence ($t_{i-w}$ to $t_i$)                  |\n",
    "| **rLP** | recent learning progress ($t_{i-w}$ to $t_i$)           |\n",
    "| **SC**  | self-challenge                                          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T17:46:43.907562Z",
     "start_time": "2021-05-16T17:44:18.758633Z"
    },
    "code_folding": [
     0,
     11
    ],
    "hidden": true,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def rlp_func(x, w, abs_lp=False):\n",
    "    '''\n",
    "    Compute recent LP in x. The resulting value depends on sizes of 2 windows equal to `m` and `n`.\n",
    "    LP is equal to the absolute difference between average score over the first and the second window.\n",
    "    The first window spans a subsequence of x from the beginning of x to m, i.e. x[:m]\n",
    "    The second windon spans a subsequence of x from the end of x to -n, i.e.  x[-n:]\n",
    "    '''\n",
    "    diff = np.mean(x[-9:]) - np.mean(x[:10])\n",
    "    return np.abs(diff) if abs_lp else diff\n",
    "\n",
    "\n",
    "def make_heuristics_dataset(input_data_path, save_path='', **kwargs):\n",
    "    # Read clean data and drop unused data\n",
    "    df = pd.read_csv(input_data_path, index_col=None).set_index(['sid','activity'])\n",
    "    df = df.loc[:, 'group,stage,trial,correct'.split(',')]\n",
    "    df = df.loc[df.trial <= 60+250]\n",
    "\n",
    "    # Add new columns\n",
    "    activities = 'A1,A2,A3,A4'.split(',')\n",
    "    for heuristic in ['pc','rpc','rlp']:\n",
    "        for a in activities:\n",
    "            df['{}{}'.format(heuristic, a[1])] = np.nan\n",
    "    df['sc'] = np.nan\n",
    "\n",
    "    # Calculate dynamic performance heuristics for each subject\n",
    "    act_codes = {'A1':1, 'A2':2, 'A3':3, 'A4':4}\n",
    "    for i, sdf in tqdm(df.groupby('sid'), desc='Progress'):\n",
    "        for a in activities:\n",
    "            x = sdf.loc[(i, a), 'correct'].astype(int)\n",
    "\n",
    "            # Overall competence (pc)\n",
    "            pc = np.cumsum(x) / np.arange(1, x.size+1)\n",
    "            df.loc[(i, a), 'pc{}'.format(a[1])] = pc\n",
    "\n",
    "            # Recent competence (rpc)\n",
    "            rpc = x.rolling(min_periods=kwargs['pc_window'], window=kwargs['pc_window']).mean()\n",
    "            df.loc[(i, a), 'rpc{}'.format(a[1])] = rpc\n",
    "\n",
    "            # Recent learning progress (rlp)\n",
    "            rlp = x.rolling(min_periods=kwargs['pc_window'], window=kwargs['lp_window']).apply(\n",
    "                rlp_func, args=(kwargs['lp_window'], kwargs['lp_window']), raw=False\n",
    "            )\n",
    "            df.loc[(i, a), 'rlp{}'.format(a[1])] = rlp\n",
    "        \n",
    "        df.loc[(i, slice(None)), :] = df.loc[(i, slice(None)), :].fillna(method='ffill', axis=0)\n",
    "\n",
    "        # Self-challenge (sc)\n",
    "        rpc_max = df.loc[(i, slice(None)), 'rpc1':'rpc4'].max(axis=1).rolling(min_periods=1, window=250).max()\n",
    "        rpc_min = df.loc[(i, slice(None)), 'rpc1':'rpc4'].min(axis=1).rolling(min_periods=1, window=250).min()\n",
    "        act_inds = np.array([act_codes[a] for a in sdf.index.get_level_values(1).tolist()]) - 1\n",
    "        current_rpc = df.loc[(i, slice(None)), 'rpc1':'rpc4'].values[np.arange(sdf.shape[0]), act_inds]\n",
    "        sc = 1 - (current_rpc-rpc_min)/(rpc_max-rpc_min)\n",
    "        df.loc[(i, slice(None)), 'sc'] = sc\n",
    "\n",
    "    df = df.reset_index().sort_values(by=['sid', 'trial'])\n",
    "    df.loc[df.stage=='train', 'sc'] = np.nan    # SC is not defined in familizarization stage\n",
    "    display(df.loc[(df.sid == 5) & (df.trial >= 60) & (df.trial < 90), :])    # Display data excerpt\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "    \n",
    "make_heuristics_dataset(\n",
    "    input_data_path = 'data/clean_data.csv',\n",
    "    save_path = 'data/heuristics_data.csv',\n",
    "    pc_window = 15,\n",
    "    lp_window = 15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Q-learning and RPE approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-29T23:22:57.466617Z",
     "start_time": "2020-11-29T23:22:35.521082Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def q_func(outcomes, init, lrate):\n",
    "    opes = [outcomes[1] - init]\n",
    "    qs = [init + lrate*(outcomes[1] - init)]\n",
    "    for i, o in enumerate(outcomes[1:]):\n",
    "        ope = o - qs[i]\n",
    "        q = qs[i] + lrate*(ope)\n",
    "        qs.append(q)\n",
    "        opes.append(ope)\n",
    "    return np.array(qs), np.array(opes)\n",
    "\n",
    "\n",
    "def make_heuristics_dataset(input_data_path, save_path, init_q, lrate, ope_smoothing, **kwargs):\n",
    "    # Read clean data and drop unused data\n",
    "    df = pd.read_csv(input_data_path, index_col=None).set_index(['sid','activity'])\n",
    "    df = df.loc[:, 'group,stage,trial,correct'.split(',')]\n",
    "    df = df.loc[df.trial <= 60+250]\n",
    "\n",
    "    # Add new columns\n",
    "    activities = 'A1,A2,A3,A4'.split(',')\n",
    "    for heuristic in ['conf','ope']:\n",
    "        for a in activities:\n",
    "            df['{}{}'.format(heuristic, a[1])] = np.nan\n",
    "\n",
    "    # Calculate dynamic performance heuristics for each subject\n",
    "    act_codes = {'A1':1, 'A2':2, 'A3':3, 'A4':4}\n",
    "    for i, sdf in tqdm(df.groupby('sid'), desc='Progress'):\n",
    "        for a in activities:\n",
    "            x = sdf.loc[(i, a), 'correct'].astype(int)\n",
    "\n",
    "            # Compute confidence and outcome prediction errors using Q-updating\n",
    "            conf, opes = q_func(x.values.squeeze(), init_q, lrate)\n",
    "            df.loc[(i, a), 'conf{}'.format(a[1])] = conf\n",
    "\n",
    "            # Recent learning progress (rlp)\n",
    "            opes = np.abs(pd.Series(opes).rolling(window=ope_smoothing, min_periods=1).mean()).values\n",
    "            df.loc[(i, a), 'ope{}'.format(a[1])] = opes\n",
    "        \n",
    "        df.loc[(i, slice(None)), :] = df.loc[(i, slice(None)), :].fillna(method='ffill', axis=0)\n",
    "\n",
    "    df = df.reset_index().sort_values(by=['sid', 'trial'])\n",
    "    display(df.loc[(df.sid == 0) & (df.trial >= 1) & (df.trial < 70), :])    # Display data excerpt\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "    \n",
    "make_heuristics_dataset(\n",
    "    input_data_path = 'data/clean_data.csv',\n",
    "    save_path = 'data/heuristics_data_alt.csv',\n",
    "    init_q = .5,\n",
    "    lrate = .2,\n",
    "    ope_smoothing = 6,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# NAM designation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## NAM dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T17:47:15.324240Z",
     "start_time": "2021-05-16T17:47:14.512436Z"
    },
    "code_folding": [
     0
    ],
    "hidden": true,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_mps(df, **kwargs):\n",
    "    '''Find mastery points'''\n",
    "    arr = df.values\n",
    "    mask = (arr != 0)\n",
    "    arr = np.where(mask.any(axis=0), mask.argmax(axis=0), kwargs['invalid_val'])\n",
    "    return pd.Series(arr, dtype=kwargs['dtype'])\n",
    "\n",
    "\n",
    "def make_nam_dataset(input_data_path, save_path, **kwargs):\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_data_path, index_col='sid')\n",
    "\n",
    "    # Select free-play trials\n",
    "    df = df.loc[(df.trial <= 60+250) & (df.trial >= 60)]\n",
    "    df.loc[:, 'trial'] -= 60\n",
    "    \n",
    "    # Get group dataset\n",
    "    group_df = df.groupby('sid').head(1)[['group']]\n",
    "\n",
    "    # Evaluate each trial's recent PC to True if mastery criterion was reached\n",
    "    mastered = df.reset_index().set_index(['sid','trial']).loc[:, 'rpc1':'rpc3'] >= kwargs['crit']\n",
    "    \n",
    "    # For each subject, find mastery points and NAM\n",
    "    by_sid = mastered.groupby('sid')\n",
    "    mastery_points = by_sid.apply(get_mps, invalid_val=250, dtype='int')\n",
    "    mastery_points.rename(columns={0:'mp1', 1:'mp2', 2:'mp3'}, inplace=True)\n",
    "#     mastery_points.replace(to_replace=0, value=15, inplace=True)\n",
    "    nam = by_sid.any().sum(axis=1).to_frame(name='nam')\n",
    "\n",
    "    # Display output dataset excerpt\n",
    "    nam = group_df.merge(nam, on='sid')\n",
    "    nam_df = nam.merge(mastery_points, on='sid').reset_index()\n",
    "    display(nam_df.head(10))\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        nam_df.to_csv(save_path, index=False)    \n",
    "\n",
    "\n",
    "make_nam_dataset(\n",
    "    input_data_path = 'data/heuristics_data.csv', \n",
    "    save_path = 'data/nam_data.csv',\n",
    "    crit = 13/15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Mastery as a function criterion dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-21T17:49:57.924104Z",
     "start_time": "2021-05-21T17:49:57.001976Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_nam_dataset(input_data_path, save_path):\n",
    "    # Load data\n",
    "    df = pd.read_csv(input_data_path, index_col='sid')\n",
    "\n",
    "    # Select free-play trials\n",
    "    df = df.loc[(df.trial <= 60+250) & (df.trial >= 60)]\n",
    "    df.loc[:, 'trial'] -= 60\n",
    "    \n",
    "    # Get group dataset\n",
    "    group_df = df.groupby('sid').head(1)[['group']]\n",
    "\n",
    "    # For different critera, calculate if mastery points were reached\n",
    "    dfs = []\n",
    "    for crit in [10,11,12,13,14]:\n",
    "        mastered = df.reset_index().set_index(['sid','trial']).loc[:, 'rpc1':'rpc3'] >= crit/15\n",
    "        mastered = mastered.groupby('sid').any()\n",
    "        mastered.rename(columns={'rpc1':'mp_1', 'rpc2':'mp_2', 'rpc3':'mp_3'}, inplace=True)\n",
    "        mastered = pd.wide_to_long(mastered.reset_index(), 'mp', i='sid', j='activity', sep='_')\n",
    "        mastered = mastered.sort_values(by=['sid', 'activity']).astype(int)\n",
    "        mastered = mastered.rename(columns={'mp':f'crit_{crit}'})\n",
    "        dfs.append(mastered)\n",
    "    \n",
    "    # Join dfs\n",
    "    df = pd.concat(dfs, axis=1).reset_index()\n",
    "    df = pd.wide_to_long(df, 'crit', i=['sid','activity'], j='crit_val', sep='_').reset_index()\n",
    "    df = df.merge(group_df, on='sid').rename(columns={'crit':'mastery'})\n",
    "    df = df.filter(items=['sid','group','activity','crit_val','mastery'])\n",
    "    display(df.sort_values(by=['sid','activity','crit_val']).head(10))\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)    \n",
    "\n",
    "\n",
    "make_nam_dataset(\n",
    "    input_data_path = 'data/heuristics_data.csv', \n",
    "    save_path = 'data/mcrits.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Learning dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T17:47:29.225751Z",
     "start_time": "2021-05-16T17:47:22.736983Z"
    },
    "code_folding": [],
    "hidden": true,
    "run_control": {
     "marked": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def make_learning_dataset(heuristics_data_path, nam_data_path, save_path='', **kwargs):    \n",
    "    # Load heuristics data\n",
    "    df = pd.read_csv(heuristics_data_path, index_col='sid')\n",
    "    \n",
    "    # Add NAM classification\n",
    "    df = df.merge(pd.read_csv(nam_data_path, index_col='sid').drop(columns='group'), on='sid')\n",
    "    \n",
    "    # Annotate switch trials\n",
    "    df['switch'] = 0\n",
    "    choices = df.activity.values\n",
    "    switches = df.switch.values.copy() \n",
    "    switches[1:] = choices[:-1] != choices[1:]\n",
    "    df.loc[:, 'switch'] = switches\n",
    "    df.loc[df.stage=='train', 'switch'] = 0  # changing activity during forced stage is not switching\n",
    "    df.loc[df.trial==61, 'switch'] = 0       # choosing activity for the first time is not switching\n",
    "    \n",
    "    # Select free-play trials\n",
    "    df = df.loc[(df.trial <= 60+250) & (df.trial > 60)]\n",
    "    df = df.loc[df.trial > 60]\n",
    "    df.loc[:, 'trial'] -= 61\n",
    "    \n",
    "    # For each subject, compute learning stats from free play stage\n",
    "    df.reset_index(inplace=True)\n",
    "    outdf = []\n",
    "    for i, sdf in tqdm(df.groupby('sid'), desc='Progress: '):\n",
    "        _sdf = sdf.set_index('trial') # index subject data by trial\n",
    "        last = _sdf.shape[0] - 1\n",
    "    \n",
    "        # Get subject information (group, nam, mps) as a pandas Series\n",
    "        profile = _sdf.head(1)[['group', 'nam', 'mp1', 'mp2', 'mp3']].iloc[0]\n",
    "\n",
    "        # Get intervals between consecutive mastery points\n",
    "        mps = profile['mp1':'mp3'].values\n",
    "        sorted_lep_bounds = np.sort(np.unique([0] + mps.tolist() + [250]))  \n",
    "        lep_intervals = pd.IntervalIndex.from_arrays(sorted_lep_bounds[:-1], sorted_lep_bounds[1:], closed='right')\n",
    "\n",
    "        # Get intervals between consecutive swiches\n",
    "        switch_trials = _sdf.switch.values.nonzero()[0].tolist() + [250]\n",
    "        if switch_trials[0] != 1: switch_trials.insert(0,1)\n",
    "\n",
    "        # Calculate self-challenge (SC) summaries\n",
    "        sc = _sdf.sc\n",
    "        sc_flat = np.mean(sc)\n",
    "        sc_lep = sc.groupby(pd.cut(_sdf.index.astype(int), lep_intervals)).mean().mean()   \n",
    "\n",
    "        # Calculate weighted initial (dwipc) and final (dwfpc) performances (+ flat performances)\n",
    "        dwipc = (_sdf.loc[0, 'rpc1':'rpc3'].values * kwargs['difficulty_weights']).sum()\n",
    "        dwfpc = (_sdf.loc[last, 'rpc1':'rpc3'].values * kwargs['difficulty_weights']).sum()\n",
    "        ipc = _sdf.loc[0, 'rpc1':'rpc3'].sum()/3\n",
    "        fpc = _sdf.loc[last, 'rpc1':'rpc3'].sum()/3\n",
    "        \n",
    "        # Get profile info and see if subject mastered activities in order of difficulty\n",
    "        sid = i\n",
    "        group = profile['group']\n",
    "        nam = profile['nam']\n",
    "        progressive = (np.diff(np.array([1,2,3])[np.argsort(mps)]) == 1).all()\n",
    "        \n",
    "        # Store subject's learning stats\n",
    "        outdf.append(\n",
    "            pd.Series(\n",
    "                data = [sid,group,nam,progressive,dwipc,dwfpc,ipc,fpc,sc_flat,sc_lep], \n",
    "                index='sid,group,nam,progressive,dwipc,dwfpc,ipc,fpc,sc_flat,sc_lep'.split(',')\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    outdf = pd.DataFrame(outdf).sort_values(by=['group','sid'])\n",
    "    display(outdf.head())\n",
    "    display(outdf.groupby('group').mean())\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        outdf.to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "make_learning_dataset(\n",
    "    heuristics_data_path = 'data/heuristics_data.csv',\n",
    "    nam_data_path = 'data/nam_data.csv',\n",
    "    save_path = 'data/learning_data.csv',\n",
    "    difficulty_weights = np.array([1,2,3])/6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Choice-modeling dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T17:47:44.992155Z",
     "start_time": "2021-05-16T17:47:35.897020Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prep_modeling_data(heuristics_data_path, nam_data_path, save_path):\n",
    "    # Load data\n",
    "    df = pd.read_csv(heuristics_data_path, index_col='sid')\n",
    "    \n",
    "    # Combine with NAM dataset\n",
    "    df = df.merge(pd.read_csv(nam_data_path, index_col='sid').drop(columns='group'), on='sid')\n",
    "    \n",
    "    # Encode activity choices as one-hot vectors\n",
    "    activity_codes = df.activity.str.get(1)\n",
    "    df = pd.concat([df, pd.get_dummies(activity_codes, prefix='ch', prefix_sep='')], axis = 1)\n",
    "    \n",
    "    add_data, act_inds = [], ['1','2','3','4']\n",
    "    for i, sdf in tqdm(df.groupby('sid'), desc='Progress'):\n",
    "        # Get relative time data\n",
    "        trials_per_activity = sdf.loc[:, 'ch1':'ch4'].cumsum(axis=0)\n",
    "        trials_total = np.tile(np.arange(trials_per_activity.shape[0]) + 1, [4, 1]).T\n",
    "        relt = trials_per_activity / trials_total\n",
    "        relt.columns = ['relt' + i for i in act_inds]\n",
    "        \n",
    "        # Get absolute time data\n",
    "        abst = trials_per_activity\n",
    "        abst.columns = ['abst' + i for i in act_inds]\n",
    "        \n",
    "        # Get previous trial choice data\n",
    "        prev = sdf.loc[:, 'ch1':'ch4']\n",
    "        prev.iloc[1:, :] = prev.iloc[:-1, :]\n",
    "        prev.iloc[0, :] = np.nan\n",
    "        prev.columns = ['prev' + i for i in act_inds]\n",
    "        \n",
    "        # Store into list\n",
    "        add_data.append([relt, prev, abst])\n",
    "    \n",
    "    add_data = pd.concat([pd.concat(h, axis=0) for h in zip(*add_data)], axis=1)\n",
    "    df = pd.concat([df, add_data], axis=1).reset_index()\n",
    "\n",
    "    # Exclude training trials\n",
    "    df = df.loc[df.trial.gt(60), :]\n",
    "    df.loc[:, 'trial'] -= 60\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "prep_modeling_data(\n",
    "    heuristics_data_path = 'data/heuristics_data.csv',\n",
    "    nam_data_path = 'data/nam_data.csv',\n",
    "    save_path = 'data/model_data.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Fitted parameters dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T19:17:57.424362Z",
     "start_time": "2021-05-16T19:17:56.925066Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prep_fitted_data(fitted_models_data_path, save_path=''):\n",
    "    # Load data\n",
    "    df = pd.read_csv(fitted_models_data_path).set_index(['sid', 'vars'])\n",
    "\n",
    "    # Initialize empty dict to turn into DF\n",
    "    var_names = df.index.get_level_values(1).to_series().unique()\n",
    "    var_names = max(list(var_names), key=len).split(',')\n",
    "    col_names = ['sid', 'vars'] + var_names + ['tau']\n",
    "    df_dict = dict(zip(col_names, [[] for _ in col_names]))\n",
    "    \n",
    "    # Iterate through DF and extract model params from stored csv strings\n",
    "    for i, row in df.iterrows():\n",
    "        if row.aic is np.nan: continue\n",
    "        df_dict['sid'].append(i[0])\n",
    "        df_dict['vars'].append(i[1])\n",
    "        params = [float(p) for p in row.params.split(',')]\n",
    "        df_dict['tau'].append(params.pop())\n",
    "        vars_included = i[1].split(',')\n",
    "        for vn in var_names:\n",
    "            df_dict[vn].append(params[vars_included.index(vn)] if vn in vars_included else np.nan)\n",
    "\n",
    "    # Generate DF from df_dict and merge with initial DF\n",
    "    df = df.filter(items=['group','nam','aic']).merge(\n",
    "        right = pd.DataFrame(df_dict).set_index(['sid', 'vars']),\n",
    "        on = ['sid', 'vars']\n",
    "    ).reset_index()\n",
    "    \n",
    "    display(df.head())\n",
    "\n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "\n",
    "prep_fitted_data(\n",
    "    fitted_models_data_path = 'data/model_results/param_fits_raw.csv',\n",
    "    save_path = 'data/model_results/param_fits_clean.csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Exposure-competence dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-16T19:17:40.602206Z",
     "start_time": "2021-05-16T19:17:38.766011Z"
    },
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def prep_data(data_path, nam_data_path, save_path):\n",
    "    full_df = pd.read_csv(data_path, index_col='sid').filter(items=['activity', 'stage', 'correct'])\n",
    "    full_df = full_df.merge(pd.read_csv(nam_data_path, index_col='sid').filter(items=['nam']), on='sid')\n",
    "    display(full_df.head())\n",
    "    activities = ['A1','A2','A3','A4']\n",
    "    \n",
    "    out_dfs = []\n",
    "    for nam in [1,2,3]:\n",
    "        out_dict = dict(zip(activities, [[] for a in activities]))\n",
    "        df = full_df.loc[full_df.nam.eq(nam), :]\n",
    "        for i, sdf in df.groupby('sid'):\n",
    "            for act_ind, sub_sdf in sdf.groupby('activity'):\n",
    "                out_dict[act_ind].append(sub_sdf.loc[:, 'correct'].tolist())\n",
    "\n",
    "        for k, v in out_dict.items():\n",
    "            rect_array = lut.boolean_indexing(v, fillval=np.nan)\n",
    "            pc = np.nanmean(rect_array, axis=0)\n",
    "            data_size = pc.size\n",
    "            pad_size = 310 - data_size\n",
    "            pc = np.concatenate([pc, np.full(pad_size, np.nan)])\n",
    "            pc[data_size:] = pc[data_size-50:data_size].mean()\n",
    "            out_dict[k] = pc\n",
    "\n",
    "        df = pd.DataFrame(out_dict).fillna(method='ffill')\n",
    "        df['nam'] = nam\n",
    "        out_dfs.append(df)\n",
    "    \n",
    "    df = pd.concat(out_dfs, axis=0).filter(items=['nam']+activities)\n",
    "    display(df.head())\n",
    "    \n",
    "    # Save data\n",
    "    if save_path:\n",
    "        print('saving to {}'.format(path.abspath(save_path)))\n",
    "        df.to_csv(save_path, index=False)\n",
    "    \n",
    "    \n",
    "with np.errstate(all='ignore'):\n",
    "    prep_data(\n",
    "        data_path = 'data/clean_data.csv',\n",
    "        nam_data_path = 'data/nam_data.csv',\n",
    "        save_path = 'data/exposure_data.csv'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
